# 配置文件说明

本文档详细说明 RustFuzz 的所有配置选项。

## 📋 配置文件概述

RustFuzz 使用两个主要配置文件：

1. **config.toml** - 全局配置（LLM、预处理器、生成器等）
2. **libraries.toml** - 目标库配置（每个要测试的 crate）

## ⚙️ config.toml 详解

### [llm] - LLM 配置

```toml
[llm]
# OpenAI API 配置
openai_api_key = ""           # OpenAI API 密钥
openai_api_base = "https://api.openai.com/v1"  # API 端点
openai_model = "gpt-4"        # 使用的模型

# Ollama 本地模型配置
use_ollama = false            # 是否使用 Ollama
ollama_host = "http://localhost:11434"  # Ollama 地址
ollama_model = "codellama:13b"  # 模型名称

# Azure OpenAI 配置（可选）
use_azure = false
azure_endpoint = ""
azure_api_key = ""
azure_deployment = ""
```

**说明**：
- 只需配置一种 LLM（OpenAI 或 Ollama）
- OpenAI：需要 API Key，付费使用，质量高
- Ollama：本地运行，免费，但需要强大的硬件
- 推荐使用 `gpt-4` 获得最佳代码生成质量

### [preprocessor] - 预处理器配置

```toml
[preprocessor]
# 是否提取测试用例作为 API 使用示例
extract_test_cases = true

# 是否提取文档示例代码
extract_doc_examples = true

# 是否特别分析 unsafe 代码块
analyze_unsafe_blocks = true

# 是否分析 panic 点
analyze_panic_points = true

# 是否导出 API 关联性为 CSV（调试用）
dump_relevance_as_csv = false

# 是否导出调用图（调试用）
dump_call_graph = false

# 是否运行类型检查
run_type_check = true
```

**说明**：
- `extract_test_cases`: 从测试代码学习 API 使用模式
- `analyze_unsafe_blocks`: unsafe 是漏洞高发区，建议开启
- `dump_*`: 用于调试，会生成大量文件，日常使用建议关闭

### [comprehender] - 理解器配置

```toml
[comprehender]
# 用于 RAG 嵌入的 LLM
embedding_llm = "text-embedding-ada-002"

# 用于理解的 LLM（留空则使用默认 LLM）
comprehension_llm = ""

# RAG 检索时返回的文档数量
retrieve_top_k = 3

# 每批处理的函数数量
function_batch_size = 12

# 是否理解 trait 约束
comprehend_trait_bounds = true

# 是否理解生命周期约束
comprehend_lifetimes = true
```

**说明**：
- `retrieve_top_k`: 数值越大，上下文越丰富，但 token 消耗越多
- `function_batch_size`: 根据 LLM 的 token 限制调整

### [generator] - 生成器配置

```toml
[generator]
# 用于生成的 LLM（留空则使用默认 LLM）
generation_llm = ""

# 收集信息时的遍历深度
collect_depth = 3

# 如果有多个同名定义，是否只选择一个
collect_one_def_in_same_names = true

# 每个 fuzz target 包含的函数数量
function_set_size = 3

# 最大生成轮次
max_rounds = 50

# 单个 fuzz target 的最大重试次数
max_retries_per_target = 3

# 是否自动生成 Arbitrary trait 实现
generate_arbitrary = true

# 是否优先测试包含 unsafe 的函数
prioritize_unsafe = true

# 是否生成结构化输入
generate_structured_input = true

# 输入数据的最大大小（字节）
max_input_size = 4096
```

**重要参数**：
- `function_set_size`: 3-5 个函数通常效果最好
- `max_rounds`: 控制总生成数量
- `prioritize_unsafe`: 强烈建议开启，unsafe 代码最容易出问题
- `max_input_size`: 限制生成的输入大小，避免无限循环

### [fuzzer] - Fuzzing 配置

```toml
[fuzzer]
# Fuzzing 引擎
engine = "libfuzzer"  # 可选: "libfuzzer", "afl++", "honggfuzz"

# 并行 job 数量（0 表示使用 CPU 核心数）
jobs = 0

# 单个 target 的运行时间（秒，0 表示无限制）
timeout = 3600

# 每个 target 的最大总运行次数
max_total_runs = 100000000

# Sanitizer 配置
sanitizers = ["address"]  # 可选: "address", "memory", "leak", "thread"

# 是否使用覆盖率引导
use_coverage = true

# 初始语料库目录（可选）
corpus_dir = ""

# 字典文件路径（可选）
dictionary_path = ""
```

**说明**：
- `engine`: libfuzzer 是默认和推荐选项
- `jobs`: 设为 0 自动使用所有 CPU 核心
- `timeout`: 根据项目复杂度调整，建议至少 1 小时
- `sanitizers`: address 可以检测内存安全问题

### [analyzer] - 分析器配置

```toml
[analyzer]
# 是否自动去重 crash
deduplicate_crashes = true

# 是否生成 crash 复现脚本
generate_reproduction_script = true

# 是否自动最小化 crash 输入
minimize_crash_input = true

# 是否收集覆盖率信息
collect_coverage = true
```

### [statistics] - 统计配置

```toml
[statistics]
# 是否生成可视化报告
generate_visualization = true

# 是否导出详细统计数据
export_detailed_stats = true

# 报告格式
report_format = "html"  # 可选: "html", "json", "markdown"
```

### [debug] - 调试配置

```toml
[debug]
# 日志级别
log_level = "INFO"  # 可选: "DEBUG", "INFO", "WARNING", "ERROR"

# 是否保存中间结果
save_intermediate_results = true

# 是否保存生成的 prompt
save_prompts = true

# 是否保存 LLM 响应
save_llm_responses = true
```

## 📦 libraries.toml 详解

### 基本配置

```toml
[library_name]  # 库的唯一标识符
language = "rust"  # 必须是 "rust"

# Crate 路径（绝对路径或相对路径）
crate_path = "/path/to/your/crate"

# 源代码路径（相对于 crate_path）
source_paths = ["src"]

# 测试代码路径（可选）
test_paths = ["tests"]

# 输出路径（保存分析结果和生成的 fuzz target）
output_path = "output/library_name"
```

### 高级配置

```toml
# 要测试的目标模块（留空则测试所有公开模块）
target_modules = ["parser", "serializer"]

# 排除的路径
exclude_paths = ["benches", "examples"]

# Cargo features（启用特定功能）
features = ["serde", "tokio"]

# 额外的依赖（如果 fuzz target 需要）
extra_dependencies = ["arbitrary"]

# 是否包含私有函数（通常只测试公开 API）
include_private = false

# API 使用提示（帮助 LLM 生成更好的代码）
api_hints = [
    "parse() 函数用于解析输入",
    "处理前应检查输入长度"
]

# 已知的 unsafe 函数（优先测试）
priority_unsafe_functions = ["raw_pointer_access", "transmute_data"]

# 重点测试的 trait 实现
priority_traits = ["FromStr", "TryFrom"]

# 是否需要异步运行时
requires_runtime = false

# 最大函数复杂度（跳过过于复杂的函数）
max_function_complexity = 100
```

## 📝 配置示例

### 示例 1：测试 JSON 库

```toml
[my_json]
language = "rust"
crate_path = "/home/user/projects/my-json"
source_paths = ["src"]
test_paths = ["tests"]
output_path = "output/my_json"
target_modules = ["parser", "serializer"]
features = ["serde"]
api_hints = [
    "parse() 解析 JSON 字符串",
    "需要处理无效的 UTF-8",
    "注意深层嵌套的 JSON"
]
prioritize_unsafe = true
```

### 示例 2：测试图像处理库

```toml
[image_lib]
language = "rust"
crate_path = "/home/user/projects/image-lib"
source_paths = ["src"]
output_path = "output/image_lib"
target_modules = ["decode", "encode"]
features = ["png", "jpeg"]
api_hints = [
    "decode() 解码图像数据",
    "需要验证图像尺寸",
    "注意整数溢出"
]
max_input_size = 10485760  # 10MB
```

### 示例 3：测试异步网络库

```toml
[async_net]
language = "rust"
crate_path = "/home/user/projects/async-net"
source_paths = ["src"]
output_path = "output/async_net"
target_modules = ["client", "server"]
features = ["tokio"]
requires_runtime = true
priority_traits = ["Future", "Stream"]
```

## 🎯 配置最佳实践

### 1. LLM 选择

**开发阶段**：
- 使用 Ollama + codellama:13b（本地，快速迭代）

**生产使用**：
- 使用 OpenAI GPT-4（质量最高）

### 2. 函数集合大小

```toml
function_set_size = 3  # 简单 API
function_set_size = 5  # 复杂 API
function_set_size = 1  # 单函数深度测试
```

### 3. 运行时间

```toml
timeout = 300    # 5分钟 - 快速测试
timeout = 3600   # 1小时 - 日常使用
timeout = 86400  # 24小时 - 深度挖掘
```

### 4. 内存控制

```toml
max_input_size = 1024      # 1KB - 简单输入
max_input_size = 4096      # 4KB - 默认
max_input_size = 1048576   # 1MB - 大数据处理
```

## 🔍 常见配置问题

### Q: 如何加快生成速度？
**A**: 
```toml
[generator]
function_set_size = 1  # 减少函数数量
collect_depth = 2       # 减少收集深度
```

### Q: 如何提高生成质量？
**A**:
```toml
[llm]
openai_model = "gpt-4"  # 使用更强的模型

[generator]
max_retries_per_target = 5  # 增加重试次数
```

### Q: 如何减少 token 消耗？
**A**:
```toml
[comprehender]
retrieve_top_k = 1  # 减少检索数量

[generator]
collect_one_def_in_same_names = true  # 只收集一个定义
```

### Q: 如何专注测试 unsafe 代码？
**A**:
```toml
[preprocessor]
analyze_unsafe_blocks = true

[generator]
prioritize_unsafe = true

[libraries.your_lib]
priority_unsafe_functions = ["func1", "func2"]
```

## 📊 配置模板

完整的配置模板见项目根目录：
- `config.toml` - 全局配置模板
- `libraries.toml` - 库配置模板
- `examples/serde_json_config.toml` - 实际项目示例

## 💡 配置建议

1. **首次使用**: 使用默认配置，不做修改
2. **熟悉后**: 根据项目特点调整参数
3. **遇到问题**: 启用 debug 模式查看详细日志
4. **优化性能**: 根据硬件和需求平衡质量与速度

---

配置完成后，你就可以开始使用 RustFuzz 了！

**下一步**: 查看 [快速开始指南](01-快速开始.md) 或 [完整教程](02-完整教程.md)
